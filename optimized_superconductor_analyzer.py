#!/usr/bin/env python3
"""
å„ªåŒ–ç‰ˆé€²éšè¶…å°é«”åˆ†æå™¨ - ä¸¦è¡Œè™•ç†å¯¦ç¾
å¯¦ç¾ä¸¦è¡Œç‰¹å¾µæå–å’Œæ€§èƒ½å„ªåŒ–
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor
import multiprocessing as mp
from functools import partial
import time
import os
from advanced_superconductor_analyzer import AdvancedSuperconductorAnalyzer

class OptimizedSuperconductorAnalyzer(AdvancedSuperconductorAnalyzer):
    """å„ªåŒ–ç‰ˆè¶…å°é«”åˆ†æå™¨ï¼Œæ”¯æŒä¸¦è¡Œè™•ç†"""
    
    def __init__(self, filename, use_parallel=True, max_workers=None):
        """åˆå§‹åŒ–å„ªåŒ–ç‰ˆåˆ†æå™¨"""
        super().__init__(filename)
        self.use_parallel = use_parallel
        self.max_workers = max_workers or min(mp.cpu_count(), 8)
        self.chunk_size = 1000  # æ¯å€‹è™•ç†å¡Šçš„å¤§å°
        
    def extract_enhanced_features_parallel(self):
        """ä¸¦è¡Œç‰ˆæœ¬çš„å¢å¼·ç‰¹å¾µæå–"""
        print("\n=== Step 2: Parallel Enhanced Feature Extraction ===")
        
        if not self.use_parallel or len(self.y_field_values) < 4:
            # å¦‚æœä¸ä½¿ç”¨ä¸¦è¡Œæˆ–æ•¸æ“šé‡å¤ªå°ï¼Œä½¿ç”¨åŸå§‹æ–¹æ³•
            return self.extract_enhanced_features()
        
        start_time = time.time()
        
        # å°‡y_fieldå€¼åˆ†çµ„ï¼Œæº–å‚™ä¸¦è¡Œè™•ç†
        n_groups = min(self.max_workers, len(self.y_field_values))
        group_size = len(self.y_field_values) // n_groups
        
        y_field_groups = []
        for i in range(n_groups):
            start_idx = i * group_size
            if i == n_groups - 1:  # æœ€å¾Œä¸€çµ„åŒ…å«å‰©é¤˜çš„æ‰€æœ‰å€¼
                end_idx = len(self.y_field_values)
            else:
                end_idx = (i + 1) * group_size
            y_field_groups.append(self.y_field_values[start_idx:end_idx])
        
        print(f"ğŸ”„ Parallel processing with {n_groups} workers")
        print(f"ğŸ”„ Group sizes: {[len(group) for group in y_field_groups]}")
        
        # ä½¿ç”¨é€²ç¨‹æ± é€²è¡Œä¸¦è¡Œè™•ç†
        with ProcessPoolExecutor(max_workers=n_groups) as executor:
            # ç‚ºæ¯å€‹é€²ç¨‹æº–å‚™æ•¸æ“šå’Œè™•ç†å‡½æ•¸
            process_func = partial(self._process_y_field_group, 
                                 data=self.data.copy())
            
            # æäº¤æ‰€æœ‰ä»»å‹™
            futures = [executor.submit(process_func, group) for group in y_field_groups]
            
            # æ”¶é›†çµæœ
            all_features = []
            for i, future in enumerate(futures):
                try:
                    group_features = future.result(timeout=300)  # 5åˆ†é˜è¶…æ™‚
                    all_features.extend(group_features)
                    print(f"âœ… Group {i+1}/{n_groups} completed ({len(group_features)} features)")
                except Exception as e:
                    print(f"âŒ Group {i+1} failed: {e}")
                    # å›é€€åˆ°åºåˆ—è™•ç†
                    return self.extract_enhanced_features()
        
        # åˆä½µæ‰€æœ‰ç‰¹å¾µ
        if all_features:
            self.features = pd.concat(all_features, ignore_index=True)
            print(f"âœ… Parallel feature extraction completed")
            print(f"âœ… Extracted features dimensions: {self.features.shape}")
            print(f"ğŸ“Š Total features: {len(self.features.columns)-1}")
        else:
            print("âŒ Parallel processing failed, falling back to sequential")
            return self.extract_enhanced_features()
        
        processing_time = time.time() - start_time
        print(f"â±ï¸  Parallel processing time: {processing_time:.2f} seconds")
        
        return self.features
    
    @staticmethod
    def _process_y_field_group(y_field_group, data):
        """è™•ç†å–®å€‹y_fieldçµ„çš„éœæ…‹æ–¹æ³•ï¼ˆç”¨æ–¼ä¸¦è¡Œè™•ç†ï¼‰"""
        features_list = []
        
        for y_field in y_field_group:
            # æå–è©²y_fieldçš„æ•¸æ“š
            field_data = data[data['y_field'] == y_field].copy()
            
            if len(field_data) < 5:  # æ•¸æ“šé»å¤ªå°‘ï¼Œè·³é
                continue
            
            # ç¢ºä¿æ•¸æ“šæŒ‰é›»æµæ’åº
            field_data = field_data.sort_values('appl_current')
            
            current = field_data['appl_current'].values
            voltage_col = [col for col in field_data.columns if 'voltage' in col][0]
            voltage = field_data[voltage_col].values
            dv_di = field_data['dV_dI'].values
            
            # è¨ˆç®—ç‰¹å¾µ
            features = OptimizedSuperconductorAnalyzer._extract_features_for_curve(
                current, voltage, dv_di, y_field
            )
            
            if features:
                features_list.append(features)
        
        # è½‰æ›ç‚ºDataFrame
        if features_list:
            return [pd.DataFrame(features_list)]
        else:
            return []
    
    @staticmethod
    def _extract_features_for_curve(current, voltage, dv_di, y_field):
        """ç‚ºå–®æ¢æ›²ç·šæå–ç‰¹å¾µçš„éœæ…‹æ–¹æ³•"""
        try:
            features = {'y_field': y_field}
            
            # 1. åŸºæœ¬çµ±è¨ˆç‰¹å¾µ
            features['V_mean'] = np.mean(voltage)
            features['V_std'] = np.std(voltage)
            features['V_max'] = np.max(voltage)
            features['V_min'] = np.min(voltage)
            features['V_range'] = features['V_max'] - features['V_min']
            
            features['dVdI_mean'] = np.mean(dv_di)
            features['dVdI_std'] = np.std(dv_di)
            features['dVdI_max'] = np.max(dv_di)
            features['dVdI_min'] = np.min(dv_di)
            
            # 2. è‡¨ç•Œé›»æµä¼°ç®—ï¼ˆç°¡åŒ–ç‰ˆï¼‰
            try:
                # æ‰¾åˆ°dV/dIçš„æœ€å¤§å€¼ä½ç½®ä½œç‚ºè‡¨ç•Œé›»æµçš„è¿‘ä¼¼
                ic_idx = np.argmax(dv_di)
                features['Ic_from_max_dVdI'] = abs(current[ic_idx])
                
                # ä½¿ç”¨é–¾å€¼æ–¹æ³•
                voltage_threshold = features['V_max'] * 0.1
                threshold_indices = np.where(np.abs(voltage) > voltage_threshold)[0]
                if len(threshold_indices) > 0:
                    features['Ic_threshold'] = abs(current[threshold_indices[0]])
                else:
                    features['Ic_threshold'] = features['Ic_from_max_dVdI']
                
                # å¹³å‡è‡¨ç•Œé›»æµ
                features['Ic_average'] = (features['Ic_from_max_dVdI'] + features['Ic_threshold']) / 2
            except:
                features['Ic_from_max_dVdI'] = np.nan
                features['Ic_threshold'] = np.nan
                features['Ic_average'] = np.nan
            
            # 3. æ³•å‘é›»é˜»ï¼ˆç°¡åŒ–ç‰ˆï¼‰
            try:
                high_current_idx = np.where(np.abs(current) > features['Ic_average'] * 2)[0]
                if len(high_current_idx) > 0:
                    features['Rn'] = np.mean(np.abs(voltage[high_current_idx] / current[high_current_idx]))
                else:
                    features['Rn'] = np.abs(voltage[-1] / current[-1]) if current[-1] != 0 else np.nan
            except:
                features['Rn'] = np.nan
            
            # 4. è½‰æ›å¯¬åº¦
            try:
                ic_10 = features['Ic_average'] * 0.1
                ic_90 = features['Ic_average'] * 0.9
                features['transition_width'] = ic_90 - ic_10
            except:
                features['transition_width'] = np.nan
            
            # 5. åŠŸç‡ç‰¹å¾µ
            power = voltage * current
            features['power_max'] = np.max(np.abs(power))
            features['power_mean'] = np.mean(np.abs(power))
            
            # 6. é »è­œç‰¹å¾µï¼ˆç°¡åŒ–ç‰ˆï¼‰
            try:
                voltage_fft = np.fft.fft(voltage)
                power_spectrum = np.abs(voltage_fft)
                features['spectral_centroid'] = np.sum(np.arange(len(power_spectrum)) * power_spectrum) / np.sum(power_spectrum)
                features['spectral_spread'] = np.sqrt(np.sum((np.arange(len(power_spectrum)) - features['spectral_centroid'])**2 * power_spectrum) / np.sum(power_spectrum))
            except:
                features['spectral_centroid'] = np.nan
                features['spectral_spread'] = np.nan
            
            return features
            
        except Exception as e:
            print(f"Warning: Feature extraction failed for y_field {y_field}: {e}")
            return None
    
    def create_performance_comparison(self, original_analyzer=None):
        """å‰µå»ºæ€§èƒ½æ¯”è¼ƒå ±å‘Š"""
        if original_analyzer is None:
            return
        
        print("\n=== Performance Comparison ===")
        
        # æ¯”è¼ƒè™•ç†æ™‚é–“
        if hasattr(self, 'processing_times') and hasattr(original_analyzer, 'processing_times'):
            orig_time = sum(original_analyzer.processing_times.values())
            opt_time = sum(self.processing_times.values())
            speedup = orig_time / opt_time if opt_time > 0 else 0
            
            print(f"Original processing time: {orig_time:.2f}s")
            print(f"Optimized processing time: {opt_time:.2f}s") 
            print(f"Speedup: {speedup:.2f}x")
            
            # å‰µå»ºæ€§èƒ½æ¯”è¼ƒåœ–
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
            
            # è™•ç†æ™‚é–“æ¯”è¼ƒ
            methods = ['Original', 'Optimized']
            times = [orig_time, opt_time]
            ax1.bar(methods, times, color=['lightcoral', 'lightgreen'])
            ax1.set_ylabel('Processing Time (seconds)')
            ax1.set_title('Processing Time Comparison')
            
            # åŠ é€Ÿæ¯”
            ax2.bar(['Speedup'], [speedup], color='gold')
            ax2.set_ylabel('Speedup Factor')
            ax2.set_title(f'Performance Improvement: {speedup:.2f}x')
            
            plt.tight_layout()
            plt.savefig('performance_comparison.png', dpi=300, bbox_inches='tight')
            print("ğŸ“Š Performance comparison saved: performance_comparison.png")

def test_optimization():
    """æ¸¬è©¦å„ªåŒ–æ•ˆæœ"""
    print("ğŸš€ TESTING OPTIMIZATION PERFORMANCE")
    print("="*50)
    
    dataset = "500.csv"  # ä½¿ç”¨ä¸­ç­‰å¤§å°çš„æ•¸æ“šé›†
    
    # æ¸¬è©¦åŸå§‹ç‰ˆæœ¬
    print("\nğŸ“Š Testing Original Analyzer...")
    start_time = time.time()
    original = AdvancedSuperconductorAnalyzer(dataset)
    original.load_and_preprocess_data()
    original.extract_enhanced_features()
    original_time = time.time() - start_time
    
    # æ¸¬è©¦å„ªåŒ–ç‰ˆæœ¬
    print("\nâš¡ Testing Optimized Analyzer...")
    start_time = time.time()
    optimized = OptimizedSuperconductorAnalyzer(dataset, use_parallel=True)
    optimized.load_and_preprocess_data()
    optimized.extract_enhanced_features_parallel()
    optimized_time = time.time() - start_time
    
    # æ¯”è¼ƒçµæœ
    print("\n" + "="*50)
    print("ğŸ“ˆ PERFORMANCE COMPARISON RESULTS")
    print("="*50)
    print(f"Original time: {original_time:.2f} seconds")
    print(f"Optimized time: {optimized_time:.2f} seconds")
    
    if optimized_time > 0:
        speedup = original_time / optimized_time
        print(f"Speedup: {speedup:.2f}x")
        
        if speedup > 1:
            print("âœ… Optimization successful!")
        else:
            print("âš ï¸  No significant speedup (overhead may dominate for this dataset size)")
    else:
        print("âŒ Optimization test failed")
    
    # é©—è­‰çµæœä¸€è‡´æ€§
    if len(original.features) == len(optimized.features):
        print("âœ… Feature count consistency verified")
    else:
        print(f"âš ï¸  Feature count mismatch: {len(original.features)} vs {len(optimized.features)}")
    
    return original_time, optimized_time

if __name__ == "__main__":
    test_optimization()
