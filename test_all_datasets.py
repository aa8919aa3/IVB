#!/usr/bin/env python3
"""
å…¨æ•¸æ“šé›†å…¼å®¹æ€§æ¸¬è©¦ - é€²éšè¶…å°é«”åˆ†æå™¨
æ¸¬è©¦æ‰€æœ‰å¯ç”¨æ•¸æ“šé›†çš„å…¼å®¹æ€§å’Œæ€§èƒ½
"""

import sys
import time
import traceback
import os
from advanced_superconductor_analyzer import AdvancedSuperconductorAnalyzer

def test_dataset(dataset_name):
    """æ¸¬è©¦å–®å€‹æ•¸æ“šé›†"""
    print(f"\nğŸ”¬ TESTING DATASET: {dataset_name}")
    print("="*50)
    
    start_time = time.time()
    
    try:
        # å‰µå»ºåˆ†æå™¨å¯¦ä¾‹
        analyzer = AdvancedSuperconductorAnalyzer(dataset_name)
        print("âœ… Analyzer initialized")
        
        # 1. æ•¸æ“šè¼‰å…¥å’Œé è™•ç†
        print("ğŸ”„ Loading and preprocessing data...")
        analyzer.load_and_preprocess_data()
        data_shape = analyzer.data.shape
        y_field_count = len(analyzer.y_field_values)
        print(f"âœ… Data: {data_shape[0]:,} points, {y_field_count} y_field values")
        
        # 2. ç‰¹å¾µæå–
        print("ğŸ”„ Extracting enhanced features...")
        analyzer.extract_enhanced_features()
        feature_count = analyzer.features.shape[1] - 1  # æ¸›å»y_fieldåˆ—
        print(f"âœ… Features: {feature_count} extracted")
        
        # 3. åœ–åƒç”Ÿæˆ
        print("ğŸ”„ Creating advanced images...")
        analyzer.create_advanced_images()
        image_count = len(analyzer.images)
        print(f"âœ… Images: {image_count} created")
        
        # 4. æ©Ÿå™¨å­¸ç¿’åˆ†æ
        print("ğŸ”„ Performing ML analysis...")
        analyzer.perform_machine_learning_analysis()
        ml_feature_count = len(analyzer.ml_features)
        print(f"âœ… ML features: {ml_feature_count} extracted")
        
        # 5. å¯è¦–åŒ–ç”Ÿæˆ
        print("ğŸ”„ Creating visualizations...")
        output_file = analyzer.create_comprehensive_visualizations()
        print(f"âœ… Visualization: {output_file}")
        
        # 6. å ±å‘Šç”Ÿæˆ
        print("ğŸ”„ Generating report...")
        analyzer.generate_comprehensive_report()
        print("âœ… Report generated")
        
        # è¨ˆç®—è™•ç†æ™‚é–“
        processing_time = time.time() - start_time
        
        # è¿”å›æ¸¬è©¦çµæœ
        results = {
            'dataset': dataset_name,
            'success': True,
            'data_points': data_shape[0],
            'y_field_values': y_field_count,
            'features_extracted': feature_count,
            'images_created': image_count,
            'ml_features': ml_feature_count,
            'processing_time': processing_time,
            'output_file': output_file,
            'error': None
        }
        
        print(f"âœ… {dataset_name} processed in {processing_time:.2f} seconds")
        return results
        
    except Exception as e:
        processing_time = time.time() - start_time
        error_msg = str(e)
        
        print(f"âŒ ERROR in {dataset_name}: {error_msg}")
        
        results = {
            'dataset': dataset_name,
            'success': False,
            'processing_time': processing_time,
            'error': error_msg,
            'traceback': traceback.format_exc()
        }
        
        return results

def run_comprehensive_dataset_test():
    """é‹è¡Œæ‰€æœ‰æ•¸æ“šé›†çš„å…¨é¢æ¸¬è©¦"""
    print("ğŸš€ COMPREHENSIVE DATASET COMPATIBILITY TEST")
    print("="*70)
    print("Testing all available datasets for compatibility and performance")
    
    # å®šç¾©è¦æ¸¬è©¦çš„æ•¸æ“šé›†
    datasets = ['164.csv', '317.csv', '500.csv']
    
    # æª¢æŸ¥æ•¸æ“šé›†æ˜¯å¦å­˜åœ¨
    available_datasets = []
    for dataset in datasets:
        if os.path.exists(dataset):
            available_datasets.append(dataset)
            print(f"âœ… Found: {dataset}")
        else:
            print(f"âŒ Missing: {dataset}")
    
    if not available_datasets:
        print("âŒ No datasets found! Please ensure CSV files are in the current directory.")
        return False
    
    print(f"\nğŸ“Š Testing {len(available_datasets)} datasets...")
    
    # æ¸¬è©¦æ‰€æœ‰å¯ç”¨æ•¸æ“šé›†
    results = []
    total_start_time = time.time()
    
    for dataset in available_datasets:
        result = test_dataset(dataset)
        results.append(result)
    
    total_time = time.time() - total_start_time
    
    # ç”Ÿæˆç¸½çµå ±å‘Š
    print("\n" + "="*70)
    print("ğŸ“Š COMPREHENSIVE TEST RESULTS SUMMARY")
    print("="*70)
    
    successful_tests = [r for r in results if r['success']]
    failed_tests = [r for r in results if not r['success']]
    
    print(f"âœ… Successful tests: {len(successful_tests)}/{len(results)}")
    print(f"âŒ Failed tests: {len(failed_tests)}")
    print(f"â±ï¸  Total processing time: {total_time:.2f} seconds")
    
    if successful_tests:
        print("\nğŸ“ˆ SUCCESSFUL DATASETS:")
        print("-" * 50)
        for result in successful_tests:
            print(f"ğŸ”¬ {result['dataset']}:")
            print(f"   ğŸ“Š Data points: {result['data_points']:,}")
            print(f"   ğŸ¯ Features: {result['features_extracted']}")
            print(f"   ğŸ–¼ï¸  Images: {result['images_created']}")
            print(f"   ğŸ¤– ML features: {result['ml_features']}")
            print(f"   â±ï¸  Time: {result['processing_time']:.2f}s")
            print(f"   ğŸ“ Output: {result['output_file']}")
    
    if failed_tests:
        print("\nâŒ FAILED DATASETS:")
        print("-" * 50)
        for result in failed_tests:
            print(f"ğŸ”¬ {result['dataset']}:")
            print(f"   âŒ Error: {result['error']}")
            print(f"   â±ï¸  Time: {result['processing_time']:.2f}s")
    
    # æ€§èƒ½åˆ†æ
    if successful_tests:
        print("\nâš¡ PERFORMANCE ANALYSIS:")
        print("-" * 50)
        
        # è¨ˆç®—æ¯å€‹æŒ‡æ¨™çš„çµ±è¨ˆæ•¸æ“š
        data_points = [r['data_points'] for r in successful_tests]
        processing_times = [r['processing_time'] for r in successful_tests]
        features = [r['features_extracted'] for r in successful_tests]
        
        avg_time = sum(processing_times) / len(processing_times)
        max_time = max(processing_times)
        min_time = min(processing_times)
        
        max_dataset = successful_tests[processing_times.index(max_time)]['dataset']
        min_dataset = successful_tests[processing_times.index(min_time)]['dataset']
        
        print(f"ğŸ“Š Dataset size range: {min(data_points):,} - {max(data_points):,} points")
        print(f"â±ï¸  Processing time: {min_time:.2f}s - {max_time:.2f}s (avg: {avg_time:.2f}s)")
        print(f"ğŸš€ Fastest: {min_dataset} ({min_time:.2f}s)")
        print(f"ğŸŒ Slowest: {max_dataset} ({max_time:.2f}s)")
        print(f"ğŸ¯ Feature extraction: {min(features)} - {max(features)} features")
    
    # å…¼å®¹æ€§å ±å‘Š
    print("\nğŸ”— COMPATIBILITY STATUS:")
    print("-" * 50)
    if len(successful_tests) == len(results):
        print("âœ… FULL COMPATIBILITY: All datasets processed successfully!")
        print("ğŸ‰ The analyzer is production-ready for all available datasets.")
    else:
        print(f"âš ï¸  PARTIAL COMPATIBILITY: {len(successful_tests)}/{len(results)} datasets successful")
        print("ğŸ”§ Some datasets may require additional handling or debugging.")
    
    return len(failed_tests) == 0

if __name__ == "__main__":
    success = run_comprehensive_dataset_test()
    
    print("\n" + "="*70)
    if success:
        print("ğŸ‰ ALL DATASET TESTS PASSED!")
        print("âœ… Ready for production deployment")
    else:
        print("âš ï¸  SOME TESTS FAILED")
        print("ğŸ”§ Review errors and implement fixes")
    
    print("="*70)
    sys.exit(0 if success else 1)
